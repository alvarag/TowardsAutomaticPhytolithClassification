{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uDt5i7gasMs"
   },
   "source": [
    "# Clasificación de fitolítos\n",
    "\n",
    "Phytolith classification\n",
    "\n",
    "This notebook is an experimental code that performs feature extraction and classification of Phytoliths. The raw images are found in the repository of a previous work. https://github.com/alvarag/AutomaticPhytolithClassification\n",
    "\n",
    "The notebook runs on Colab and saves the data to drive. It is possible to run it locally.\n",
    "\n",
    "Saves the datasets, resulting from the processes of extracting features from the images.\n",
    "- Saved datasets are Pandas DataFrames, which contain:\n",
    "    - Image file name.\n",
    "    - Phytolith class name.\n",
    "    - An array with 10 \"test fold ids\". In this experiment we are doing 10 x 10 cross validation. For each image, the test fold in which the image is in each repetition is stored. It makes it possible to ensure that all the methods have been evaluated in the same way (trained and tested always with the same sets of images).\n",
    "\n",
    "- Dictionary of results. A dictionary with a key (IdAtributes, IdClassifier), and as values dictionaries that contain all the predictions made in each of the folds of each of the repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0LYr5DlJoGH"
   },
   "source": [
    "# Instalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbMtDWA5M4WV",
    "outputId": "7217fb3c-fbe2-491e-8bab-51f2e7e7bb42"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Experimental. \n",
    "Installing pandarallel to parallelize feature extraction\n",
    "https://towardsdatascience.com/pandaral-lel-a-simple-and-efficient-tool-to-parallelize-your-pandas-operations-on-all-your-cpus-bb5ff2a409ae\n",
    "Not used in this version\n",
    "'''\n",
    "!pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "from IPython.display import clear_output\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "clear_output()\n",
    "print(\"Installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9gHw2azuT4q",
    "outputId": "40413791-e3d2-4bed-fe16-1bbd785651ec"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Installing pyedf, for computing elliptic fourier descriptors\n",
    "'''\n",
    "!pip install pyefd\n",
    "from pyefd import elliptic_fourier_descriptors\n",
    "clear_output()\n",
    "print(\"Installed\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urZ9gQzzAwZS",
    "outputId": "6b43e008-24a7-4e23-c3e8-2cb9bcfa51fa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Installing mahotas, for computing Zernique moments\n",
    "Not used in this version\n",
    "'''\n",
    "!pip install mahotas\n",
    "clear_output()\n",
    "print(\"Installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W55shxlghWGq",
    "outputId": "f8d71e27-b912-4f58-bb9a-5f72df1c5f07"
   },
   "outputs": [],
   "source": [
    "#Force to use pickle version 5 \n",
    "!pip3 install pickle5\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyuDRci1aWdd"
   },
   "source": [
    "# Imports and variables\n",
    "\n",
    "Importación de la bibliotecas y variables principales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjNvw86KkpbI"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import libraries\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt   \n",
    "\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from skimage.draw import polygon\n",
    "from skimage.measure import regionprops, find_contours, label\n",
    "from skimage.transform import resize, rotate\n",
    "from skimage.util import montage, img_as_ubyte\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage import io\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "from skimage.feature.texture import local_binary_pattern\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ4P4dDNdqOh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Configuration data\n",
    "\n",
    "csvs_path = Directory with multiple subfolders, one for each morphotype, \n",
    "each of these subfolders with a .csv file per image.\n",
    "\n",
    "imgs_path = Directory with multiple subfolders, one for each morphotype, \n",
    "each of these subfolders contains multiple images.\n",
    "\n",
    "Watch out: delimiter separator depends on the operating system.\n",
    "'''\n",
    "# change to use the notebook in local\n",
    "colab = True\n",
    "\n",
    "\n",
    "path = \"AutomaticPhytolithClassification/phytoliths\"\n",
    "\n",
    "# Path to csv files\n",
    "csvs_path = path + os.sep + \"csvs\"\n",
    "\n",
    "# Path to image files\n",
    "imgs_path = path + os.sep + \"imgs\"\n",
    "\n",
    "# Output_folder\n",
    "output_folder = \"phyto_output\"\n",
    "\n",
    "complete_output_folder_path = \"\"\n",
    "if colab:\n",
    "  complete_output_folder_path = f\"/content/drive/MyDrive/{output_folder}/\"\n",
    "else:\n",
    "  complete_output_folder_path = \".\"+os.sep\n",
    "\n",
    "# Datasets serialized file\n",
    "datasets_file = \"datasets.obj\"\n",
    "\n",
    "# Results serialized file\n",
    "results_file = \"results.obj\"\n",
    "\n",
    "# Size normalization\n",
    "target_size = (300,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_dfCvBL_yKK"
   },
   "source": [
    "# Google Drive Setup\n",
    "\n",
    "The datasets and results are saved on the drive chosen by the user. Only drives owned by the user can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFYalXyr5rDq",
    "outputId": "4351ee23-e52d-4298-f443-a2157828cd31"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "\n",
    "def mount():\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtrs2QvXJHy6"
   },
   "source": [
    "# Import datasets\n",
    "\n",
    "\n",
    "Download the images from the phytolith repository.\n",
    "\n",
    "(Modify the following cells to use the code with your own images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fcjMMvBMUZw"
   },
   "outputs": [],
   "source": [
    "# Download AutomaticPhytolithClassification repository\n",
    "!git clone https://github.com/alvarag/AutomaticPhytolithClassification.git\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PIFnBQqKnRJ"
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvoZo03I-u7W"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Features to be recalculated\n",
    "\n",
    "It allows repeating the generation of the datasets (if you want to test new parameters or modifications).\n",
    "If set to false, it does not recalculate the datasets, but retrieves them from drive.\n",
    "\n",
    "'''\n",
    "repeat_dict = {\n",
    "    \"LBP\": False, # LBP \n",
    "    \"Morpho\": False, # Morphological\n",
    "    \"EFD\": False, # Elliptic fourier desc\n",
    "    \"Hu\": False, # Hu moments\n",
    "    \"pftas\":False,\n",
    "    \"Haralick\":False,\n",
    "    \n",
    "    \"InceptionV3\":False, \"Xception\":False, \n",
    "    \"VGG16\":False, \"VGG19\":False, \n",
    "    \"ResNet50\":False, \"ResNet101\":False, \"ResNet152\":False, \n",
    "    \"InceptionResNet\":False, \"MobileNet\":False,\n",
    "    \"InceptionV3\":False, \"InceptionV3\":False, \"InceptionV3\":False,\n",
    "    \"NASMobile\":False, \"NASLarge\":False, \n",
    "    \"Dense121\":False, \"Dense169\":False, \"Dense201\":False\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4yF-Ic-ky3m"
   },
   "source": [
    "# Utility functions (main)\n",
    "\n",
    "Functions to generate data sets in DataFrame format.\n",
    "\n",
    "These functions take a string (a row of a dataframe) that has information about the name of the image etc and return another string, with the desired attributes.\n",
    "\n",
    "Using these functions together with \"apply\" you can get an features dataframe from another dataframe with the image metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv3u-HXgkGf2"
   },
   "outputs": [],
   "source": [
    "def crop_square(img, rectangle):\n",
    "  \"\"\"\n",
    "  Clips a square image that circumscribes the rectangle passed as an argument\n",
    "        \n",
    "  \"\"\"\n",
    "  (y1,x1),(y2,x2) = rectangle\n",
    "  height = y2-y1\n",
    "  width = x2-x1\n",
    "  size = max(height,width)\n",
    "  center_y, center_x = y1+int(height/2),x1+int(width/2)\n",
    "  new_y1, new_y2 = center_y-int(size/2), center_y+int(size/2)\n",
    "  new_x1, new_x2 = center_x-int(size/2), center_x+int(size/2)\n",
    "\n",
    "\n",
    "  img_detail_centered = img[new_y1:new_y2,new_x1:new_x2].copy()\n",
    "  return img_detail_centered\n",
    "\n",
    "\n",
    "def center_and_crop_square(img, rectangle):\n",
    "  \"\"\"\n",
    "  Clips a square image that circumscribes the rectangle passed as an argument\n",
    "  Center the area to be cropped \"rolling the image\"\n",
    "        \n",
    "  \"\"\"\n",
    "  (y1,x1),(y2,x2) = rectangle\n",
    "  height = y2-y1\n",
    "  width = x2-x1\n",
    "  size = max(height,width)\n",
    "  center_y, center_x = y1+int(height/2),x1+int(width/2)\n",
    "\n",
    "  img_center_y = int(img.shape[0]/2)\n",
    "  img_center_x = int(img.shape[1]/2)\n",
    "\n",
    "  diff_y, diff_x = int(img_center_y- center_y), int(img_center_x - center_x)\n",
    "\n",
    "  rolled = np.roll(img,diff_y,axis=0)\n",
    "  rolled = np.roll(rolled,diff_x,axis=1)\n",
    "\n",
    "  new_y1, new_y2 = img_center_y-int(size/2), img_center_y+int(size/2)\n",
    "  new_x1, new_x2 = img_center_x-int(size/2), img_center_x+int(size/2)\n",
    "\n",
    "  img_detail_centered = rolled[new_y1:new_y2,new_x1:new_x2].copy()\n",
    "\n",
    "  return img_detail_centered\n",
    "\n",
    "\n",
    "def crop_image(img,rectangle):\n",
    "  \"\"\"\n",
    "  Clips a circumscribed square to the rectangle passed as an argument\n",
    "   If necessary \"roll the image\"\n",
    "  \"\"\"\n",
    "  crop = crop_square(img, rectangle)\n",
    "  (y1,x1),(y2,x2) = rectangle\n",
    "  h,w = 0,0\n",
    "  if len(crop.shape)==3:\n",
    "    h,w,_ = crop.shape\n",
    "  else:\n",
    "    h,w = crop.shape\n",
    "  size = max(y2-y1,x2-x1)\n",
    "  #print(size,h,w)\n",
    "  if size<h*0.95 or size <w*0.95:\n",
    "    crop = center_and_crop_square(img, rectangle)\n",
    "\n",
    "  return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6rMaAkwtqS4"
   },
   "outputs": [],
   "source": [
    "def string_to_dict(dict_string):\n",
    "    \"\"\"\n",
    "    Convert a string to a dictionary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_string : string\n",
    "        string containing a dictionary encoded as text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionay : dict\n",
    "        A dictionary containing the same information\n",
    "    \"\"\"\n",
    "    dict_string = dict_string.replace(\"'\", '\"').replace('u\"', '\"')\n",
    "    return json.loads(dict_string)\n",
    "\n",
    "def process_csv(file):\n",
    "    \"\"\"\n",
    "    Process the csv files obtained by the image labeler    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : string\n",
    "        string containing the path to the csv file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : tuple\n",
    "        A tuple containing the bounding box coordinates, \n",
    "        the full list of contour points and the image name\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file) \n",
    "    img_name = data.filename\n",
    "    points = data.region_shape_attributes[0]\n",
    "    points_dict = string_to_dict(points)\n",
    "    \n",
    "    xs = points_dict[\"all_points_x\"]\n",
    "    ys = points_dict[\"all_points_y\"]\n",
    "    \n",
    "    coords = list(zip(ys,xs)) \n",
    "    \n",
    "    return ((min(ys),min(xs)),(max(ys),max(xs))) ,coords, img_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w7BRuyZtsSl"
   },
   "outputs": [],
   "source": [
    "def create_image_info_df(csvs_path,num_folds=10,\n",
    "                         num_repetitions = 10,\n",
    "                         ignore_classes = []):\n",
    "  \"\"\"\n",
    "  Process all csv files obtained by the image labeler.\n",
    "  Obtains a dataset with \"Image\",\"Rectangle\",\"Coords\",\"Class\", and\n",
    "  cross validation partitions for future experiments\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csvs_path : string\n",
    "        string containing the path to the csv file\n",
    "    num_folds: integer\n",
    "        number of cross validation folds\n",
    "    num_repetitions: integer\n",
    "        number of repetitions of cross validation procedure\n",
    "    ignore_classes: list\n",
    "        class names to be excluded from the dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : Dataframe\n",
    "        A dataframe containing image name, the bounding box coordinates, \n",
    "        the full list of contour points, the class and the the test partition \n",
    "        to which the image will belong\n",
    "        \n",
    "    \"\"\"\n",
    "  url = csvs_path+os.sep\n",
    "  clases = []\n",
    "  rects = []\n",
    "  coords = []\n",
    "  image_files = []\n",
    "\n",
    "  # Listing all .csv files\n",
    "  files = [f for f in glob.glob(url+\"**\"+os.sep+\"*.csv\", recursive=True)]\n",
    "\n",
    "  for file in files:    \n",
    "    \n",
    "      clases.append(file.split(os.sep)[-2])\n",
    "          \n",
    "      rect, coord, image_file = process_csv(file)\n",
    "      rects.append(rect)\n",
    "      coords.append(coord)\n",
    "      image_files.append(image_file)\n",
    "      \n",
    "  # intialise data of lists. \n",
    "  data = {'Image':image_files, \n",
    "          'Rectangle':rects,\n",
    "          'Coords': coords,\n",
    "          'Class':clases} \n",
    "    \n",
    "  # Create DataFrame \n",
    "  df = pd.DataFrame(data)[[\"Image\",\"Rectangle\",\"Coords\",\"Class\"]] \n",
    "\n",
    "  # Remove small classes\n",
    "  for class_name in ignore_classes:\n",
    "    df = df[~(df.Class==class_name)]\n",
    "\n",
    "  \n",
    "  y = df.Class\n",
    "\n",
    "  # Add test folds\n",
    "  for i in range(num_repetitions):\n",
    "\n",
    "    test_fold = np.full(len(y),-1)\n",
    "    skf = StratifiedKFold(n_splits=num_folds, \n",
    "                          shuffle=True,random_state=i)\n",
    "    \n",
    "    skf.get_n_splits(y, y)\n",
    "\n",
    "                                                                #X\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(y, y)):\n",
    "        test_fold[test_index] = fold_idx\n",
    "\n",
    "    df[f\"Test_Fold{i}\"] = test_fold\n",
    "\n",
    "  return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrCAMGrW1Q5K"
   },
   "outputs": [],
   "source": [
    "def get_mask(img,r,c):\n",
    "    \"\"\"\n",
    "    Obtains the image mask    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        The image\n",
    "    r ndarray\n",
    "        Row coordinates of vertices of polygon.\n",
    "    c ndarray\n",
    "        Column coordinates of vertices of polygon.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mask : ndarray of type ‘bool’.\n",
    "\n",
    "    The mask that corresponds to the input polygon.\n",
    "\n",
    "    \"\"\"\n",
    "    image_shape = img.shape[:-1]    \n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    rr, cc = polygon(r, c)\n",
    "    mask[rr, cc] = 1\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IqpcQZHXvrM"
   },
   "outputs": [],
   "source": [
    "def register_to_info(register):\n",
    "  img_name = register.Image\n",
    "  #print(img_name)\n",
    "  Class = register.Class    \n",
    "    \n",
    "  img_dir = imgs_path+os.sep+Class+os.sep    \n",
    "  Name_str = Class+\"_\"+img_name    \n",
    "    \n",
    "  img_path = img_dir+img_name \n",
    "    \n",
    "    \n",
    "  # the polygon function needs the rows on one var and the columns on other\n",
    "  r,c = zip(*register.Coords)\n",
    "  (y1,x1),(y2,x2) = register.Rectangle\n",
    "    \n",
    "  img = imread(img_path)\n",
    "\n",
    "  return img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee1hs-aupblr"
   },
   "outputs": [],
   "source": [
    "def get_min_max_feret(mask):\n",
    "    \"\"\"\n",
    "    Compute min_feret and max_feret    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : ndarray\n",
    "        Binary image \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    feret_max : integer \n",
    "    feret_min : integer\n",
    "\n",
    "    \"\"\"\n",
    "    feret_max = 0\n",
    "    feret_min = 999999\n",
    "\n",
    "    # the idea is to make 360 rotations and take out the size of the bounding box\n",
    "    for i in range(360):\n",
    "        mask_r = rotate(mask,i,preserve_range=True,resize=True)\n",
    "\n",
    "        label_image = label(mask_r)\n",
    "\n",
    "        region = regionprops(label_image)[0]\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        lengths = (maxc-minc, maxr - minr)\n",
    "        max_l = max(lengths)\n",
    "        min_l = min(lengths)\n",
    "\n",
    "        if max_l > feret_max:\n",
    "            feret_max = max_l\n",
    "        if min_l < feret_min:\n",
    "            feret_min = min_l\n",
    "        \n",
    "    return feret_max, feret_min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV2sMZB_0vgk"
   },
   "outputs": [],
   "source": [
    "def get_efd(mask):\n",
    "    \"\"\"\n",
    "    Compute elliptic fourier descriptors    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : ndarray\n",
    "        Binary image \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Edfs : ndarray \n",
    "        Array of elliptic fourier descriptors\n",
    "\n",
    "    \"\"\"\n",
    "    contours = find_contours(mask, 0.5)\n",
    "    \n",
    "    coeffs = elliptic_fourier_descriptors(contours[0],order=10,normalize=True)\n",
    "    return coeffs.flatten()[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvezYovO06eW"
   },
   "outputs": [],
   "source": [
    "def register_to_morpho_features(register):\n",
    "    \"\"\"\n",
    "    \n",
    "    Compute basic morphologic features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : ndarray\n",
    "        Binary image \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Edfs : ndarray \n",
    "        Array of morphologic features\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "\n",
    "    test_folds = register.filter(regex='Test_Fold')\n",
    "\n",
    "    img_gray = rgb2gray(img)\n",
    "    \n",
    "    \n",
    "    mask = get_mask(img,r,c)    \n",
    "        \n",
    "    # min feret and max feret\n",
    "    Length,Width = get_min_max_feret(mask[y1:y2,x1:x2])\n",
    "    \n",
    "    ## Properties of the mask\n",
    "    region = regionprops(mask,intensity_image=img_gray)[0]\n",
    "    Perimeter = region.perimeter\n",
    "    Area = region.area\n",
    "    ConvexArea = region.convex_area\n",
    "    MajorAxisLength = region.major_axis_length\n",
    "    MinorAxisLength = region.minor_axis_length\n",
    "    EquivDiam = region.equivalent_diameter\n",
    "    \n",
    "    ## convex hull of the mask.\n",
    "    chull = convex_hull_image(mask)\n",
    "    regionPerimConvexHull = regionprops(chull.astype(int))[0]\n",
    "    perimeterHull = regionPerimConvexHull.perimeter\n",
    "    \n",
    "    Convexity = perimeterHull/Perimeter    \n",
    "    Solidity = Area/ConvexArea\n",
    "    AspectRatio = Length/Width\n",
    "    Roundness = (4*Area*(math.pi))/((Length)**2)\n",
    "    Compactness = EquivDiam/Length\n",
    "    \n",
    "    FormFactor = (4*Area*(math.pi))/((Perimeter)**2)\n",
    "    \n",
    "    basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"]) \n",
    "    morfo_values = pd.Series([Perimeter,perimeterHull,Area,ConvexArea,\n",
    "                              MajorAxisLength,MinorAxisLength,\n",
    "                               EquivDiam,FormFactor,Length,Width,\n",
    "                               Convexity,Solidity, AspectRatio,Roundness,Compactness],\n",
    "                              [\"Perimeter\",\"PerimeterHull\",\"Area\",\"Convex Area\",\n",
    "                               \"Major axis length\",\"Minor axis length\",\n",
    "                               \"Equivalent diameter\",\"Form factor\",\"Length\",\"Width\",\n",
    "                               \"Convexity\",\"Solidity\", \"AspectRatio\",\"Roundness\",\"Compactness\"])\n",
    "        \n",
    "    return pd.concat((basic_values,test_folds,morfo_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp1OXtEpLh8D"
   },
   "outputs": [],
   "source": [
    "def register_to_efd_features(register):\n",
    "  \"\"\"\n",
    "    Compute Elliptic Fourier Descriptors    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with Elliptic Fourier Descriptors descriptors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "  test_folds = register.filter(regex='Test_Fold')\n",
    "  img_gray = rgb2gray(img)\n",
    "    \n",
    "    \n",
    "  mask = get_mask(img,r,c)    \n",
    "  efds = get_efd(mask)  \n",
    "    \n",
    "    \n",
    "    \n",
    "  basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "  efd_values = pd.Series(efds,[\"edf\"+str(i) for i in range(len(efds))])\n",
    "    \n",
    "  return pd.concat((basic_values,test_folds,efd_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV7M-kLGURWu"
   },
   "outputs": [],
   "source": [
    "def register_to_lbp_features(register,radious=3,use_mask = True):\n",
    "    \"\"\"\n",
    "    Compute Local Binary Patterns    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with Local Binary Patterns descriptors\n",
    "\n",
    "    \"\"\"\n",
    "    img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "    test_folds = register.filter(regex='Test_Fold')\n",
    "    img_gray = img_as_ubyte(rgb2gray(img))  \n",
    "    img_gray=img_gray[y1:y2,x1:x2] \n",
    "    \n",
    "    mask = get_mask(img,r,c)  \n",
    "    mask=mask[y1:y2,x1:x2] \n",
    "    \n",
    "    lbp = None\n",
    "    P = 8\n",
    "\n",
    "    dim = 2**P # Number of grey-scale levels\n",
    "\n",
    "    #h_bins = np.arange(dim+1)\n",
    "\n",
    "    h_bins = np.arange(0,dim+1,16) # Quantization levels\n",
    "    h_range = (0, dim)\n",
    "\n",
    "    \n",
    "    codes = local_binary_pattern(img_gray, P, radious, method=\"default\")\n",
    "    #print(\"codes\",codes[0])\n",
    "    \n",
    "    h_img, _ = np.histogram(codes.ravel(), bins=h_bins, range=h_range)\n",
    "    h_masked, _ = np.histogram(codes[mask], bins=h_bins, range=h_range)\n",
    "    h_img = h_img/h_img.sum(dtype=np.float)\n",
    "    h_masked = h_masked/h_masked.sum(dtype=np.float)\n",
    "\n",
    "    if use_mask:\n",
    "      lbp = h_masked.copy()\n",
    "    else:\n",
    "      lbp = h_img.copy()\n",
    "    \n",
    "    \n",
    "    basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "    lbp_values = pd.Series(lbp,[\"LBP\"+str(i) for i in range(len(lbp))])\n",
    "    \n",
    "    return pd.concat((basic_values,test_folds,lbp_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiyz7X5rAMBf"
   },
   "outputs": [],
   "source": [
    "def get_hu(img):\n",
    "  label_image = label(img)\n",
    "  regions = regionprops(label_image)\n",
    "  \n",
    "  region = regions[0]\n",
    "  hu = region.moments_hu\n",
    "  #(min_row, min_col, max_row, max_col) = region.bbox\n",
    "  return hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47y87L-KAcNx"
   },
   "outputs": [],
   "source": [
    "import mahotas\n",
    "\n",
    "'''\n",
    "Not used\n",
    "'''\n",
    "def get_zernike(img,radious):\n",
    "  label_image = label(img)\n",
    "  regions = regionprops(label_image)\n",
    "  \n",
    "  # obtengo la primera región (solo funciona bien si hay una)\n",
    "  region = regions[0]\n",
    "  (min_row, min_col, max_row, max_col) = region.bbox\n",
    "  \n",
    "  img_crop = img[min_row:max_row,min_col:max_col]\n",
    "  return mahotas.features.zernike_moments(img_crop, radious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJY5x8t0UIdA"
   },
   "outputs": [],
   "source": [
    "import mahotas\n",
    "\n",
    "\n",
    "def register_to_haralick_features(register,distance=1):\n",
    "  \"\"\"\n",
    "    Compute haralick features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with Haralick descriptors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "  test_folds = register.filter(regex='Test_Fold')\n",
    "    \n",
    "  \n",
    "  img_crop = crop_image(img,((y1,x1),(y2,x2)))\n",
    "\n",
    "  haralick_features =  mahotas.features.haralick(img_crop,distance=distance,\n",
    "                                                 return_mean=True,\n",
    "                                                 ignore_zeros = False)\n",
    "\n",
    "  basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "  haralick_values = pd.Series(haralick_features,[\"haralick\"+str(i) for i in range(len(haralick_features))])\n",
    "    \n",
    "  return pd.concat((basic_values,test_folds,haralick_values))\n",
    "\n",
    "\n",
    "\n",
    "def register_to_pftas_features(register):\n",
    "  \"\"\"\n",
    "    Compute PFTAS features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with PFTAS descriptors\n",
    "\n",
    "  \"\"\"\n",
    "  img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "  test_folds = register.filter(regex='Test_Fold')\n",
    "  \n",
    "  img_crop = crop_image(img,((y1,x1),(y2,x2)))\n",
    "\n",
    "  pftas_features =  mahotas.features.pftas(img_crop)\n",
    "\n",
    "  basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "  pftas_values = pd.Series(pftas_features,[\"pftas\"+str(i) for i in range(len(pftas_features))])\n",
    "    \n",
    "  return pd.concat((basic_values,test_folds,pftas_values))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6Mr4feJBeLR"
   },
   "outputs": [],
   "source": [
    "def register_to_hu_features(register):\n",
    "  \"\"\"\n",
    "    Compute Hu Moments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with Hu Moments descriptors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "  test_folds = register.filter(regex='Test_Fold')\n",
    "    \n",
    "  mask = get_mask(img,r,c)    \n",
    "  hu_features = get_hu(mask)  \n",
    "    \n",
    "    \n",
    "    \n",
    "  basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "  hu_values = pd.Series(hu_features,[\"hu\"+str(i) for i in range(len(hu_features))])\n",
    "    \n",
    "  return pd.concat((basic_values,test_folds,hu_values))\n",
    "\n",
    "\n",
    "def register_to_zernike_features(register,radious):\n",
    "  \"\"\"\n",
    "    Compute Zernike Moments (Not Used)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Pandas Series\n",
    "        Image Info \n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    lbps : Pandas Series \n",
    "        Serie with Zernike Moments descriptors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  img_name, Class, Name_str, (r,c), ((y1,x1),(y2,x2)), img = register_to_info(register)\n",
    "  test_folds = register.filter(regex='Test_Fold')\n",
    "    \n",
    "  mask = get_mask(img,r,c)    \n",
    "  zernike_features = get_zernike(mask,radious)  \n",
    "    \n",
    "    \n",
    "    \n",
    "  basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "  zernike_values = pd.Series(zernike_features,[\"zernike\"+str(i) for i in range(len(zernike_features))])\n",
    "    \n",
    "  return pd.concat((basic_values,test_folds,zernike_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wop-LZVg7n9b"
   },
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o5DpqTF7rWK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing, loading and configure pretrained models\n",
    "'''\n",
    "\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocessor\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocessor\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocessor\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_v2_preprocessor\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as incept_res_v2_preprocessor\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocessor\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocessor\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocessor\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.applications import NASNetMobile\n",
    "from tensorflow.keras.applications import NASNetLarge\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "models_dict = {}\n",
    "inception_v3_dict, xception_dict, vgg16_dict, vgg19_dict = {} ,{} ,{} ,{}\n",
    "resnet50_dict, resnet101_dict, resnet152_dict = {} ,{} ,{}\n",
    "incepres_dict, mobile_dict, nasmobile_dict, naslarge_dict = {} ,{} ,{}, {}\n",
    "dense121_dict, dense169_dict, dense201_dict = {} ,{} ,{}\n",
    "\n",
    " \n",
    "if repeat_dict[\"InceptionV3\"]:\n",
    "  print(\"Loading Inception v3\")\n",
    "  model = InceptionV3(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  inception_v3_dict[\"model\"] = clone_model(model)\n",
    "  inception_v3_dict[\"preprocesor\"] = inception_v3_preprocessor\n",
    "  inception_v3_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"Xception\"]:\n",
    "  print(\"Loading Xception\")\n",
    "  model = Xception(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  xception_dict[\"model\"] = clone_model(model)\n",
    "  xception_dict[\"preprocesor\"] = xception_preprocessor\n",
    "  xception_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"VGG16\"]:\n",
    "  print(\"Loading VGG16\")\n",
    "  model = VGG16(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  vgg16_dict[\"model\"] = clone_model(model)\n",
    "  vgg16_dict[\"preprocesor\"] = vgg16_preprocessor\n",
    "  vgg16_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"VGG19\"]:\n",
    "  print(\"Loading VGG19\")\n",
    "  model = VGG19(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  vgg19_dict[\"model\"] = clone_model(model)\n",
    "  vgg19_dict[\"preprocesor\"] = vgg19_preprocessor\n",
    "  vgg19_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"ResNet50\"]:\n",
    "  print(\"Loading ResNet 50\")\n",
    "  model = ResNet50V2(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  resnet50_dict[\"model\"] = clone_model(model)\n",
    "  resnet50_dict[\"preprocesor\"] = resnet_v2_preprocessor\n",
    "  resnet50_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"ResNet101\"]:\n",
    "  print(\"Loading ResNet 101\")\n",
    "  model = ResNet101V2(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  resnet101_dict[\"model\"] = clone_model(model)\n",
    "  resnet101_dict[\"preprocesor\"] = resnet_v2_preprocessor\n",
    "  resnet101_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"ResNet152\"]:\n",
    "  print(\"Loading ResNet 152\")\n",
    "  model = ResNet152V2(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  resnet152_dict[\"model\"] = clone_model(model)\n",
    "  resnet152_dict[\"preprocesor\"] = resnet_v2_preprocessor\n",
    "  resnet152_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"InceptionResNet\"]:\n",
    "  print(\"Loading InceptionResNetV2\")\n",
    "  model = InceptionResNetV2(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  incepres_dict[\"model\"] = clone_model(model)\n",
    "  incepres_dict[\"preprocesor\"] = incept_res_v2_preprocessor\n",
    "  incepres_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"MobileNet\"]:\n",
    "  print(\"Loading MobileNet\")\n",
    "  model = MobileNetV2(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  mobile_dict[\"model\"] = clone_model(model)\n",
    "  mobile_dict[\"preprocesor\"] = mobilenet_preprocessor\n",
    "  mobile_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "if repeat_dict[\"NASMobile\"]:\n",
    "  print(\"Loading NASMobile\")\n",
    "  model = NASNetMobile(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  nasmobile_dict[\"model\"] = clone_model(model)\n",
    "  nasmobile_dict[\"preprocesor\"] = nasnet_preprocessor\n",
    "  nasmobile_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"NASLarge\"]:\n",
    "  print(\"Loading NASLarge\")\n",
    "  model = NASNetLarge(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  naslarge_dict[\"model\"] = clone_model(model)\n",
    "  naslarge_dict[\"preprocesor\"] = nasnet_preprocessor\n",
    "  naslarge_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"Dense121\"]:\n",
    "  print(\"Loading Dense121\")\n",
    "  model = DenseNet121(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  dense121_dict[\"model\"] = clone_model(model)\n",
    "  dense121_dict[\"preprocesor\"] = densenet_preprocessor\n",
    "  dense121_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"Dense169\"]:\n",
    "  print(\"Loading Dense169\")\n",
    "  model = DenseNet169(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  dense169_dict[\"model\"] = clone_model(model)\n",
    "  dense169_dict[\"preprocesor\"] = densenet_preprocessor\n",
    "  dense169_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "if repeat_dict[\"Dense201\"]:\n",
    "  print(\"Loading Dense201\")\n",
    "  model = DenseNet201(weights='imagenet')\n",
    "  model = Model(model.input, model.layers[-2].output)\n",
    "  dense201_dict[\"model\"] = clone_model(model)\n",
    "  dense201_dict[\"preprocesor\"] = densenet_preprocessor\n",
    "  dense201_dict[\"target_size\"] = model.input_shape[1],model.input_shape[2]\n",
    "\n",
    "\n",
    "\n",
    "models_dict[\"InceptionV3\"] = inception_v3_dict\n",
    "models_dict[\"Xception\"] = xception_dict\n",
    "models_dict[\"VGG16\"] = vgg16_dict\n",
    "models_dict[\"VGG19\"] = vgg19_dict\n",
    "models_dict[\"ResNet50\"] = resnet50_dict\n",
    "models_dict[\"ResNet101\"] = resnet101_dict\n",
    "models_dict[\"ResNet152\"] = resnet152_dict\n",
    "models_dict[\"InceptionResNet\"] = incepres_dict\n",
    "models_dict[\"MobileNet\"] = mobile_dict\n",
    "models_dict[\"NASMobile\"] = nasmobile_dict\n",
    "models_dict[\"NASLarge\"] = naslarge_dict\n",
    "models_dict[\"Dense121\"] = dense121_dict\n",
    "models_dict[\"Dense169\"] = dense169_dict\n",
    "models_dict[\"Dense201\"] = dense201_dict\n",
    "\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8Jw9KoO8S5Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.io import imsave\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features(np_image,model_name):\n",
    "\n",
    "  model_dict = models_dict[model_name]\n",
    "  model = model_dict[\"model\"]\n",
    "  preprocessor = model_dict[\"preprocesor\"]\n",
    "  target_size = model_dict[\"target_size\"]\n",
    "\n",
    "  #Create temp file\n",
    "  tmp_path = str(uuid.uuid4())+\".png\"\n",
    "  imsave(tmp_path,np_image)\n",
    "\n",
    "  img = image.load_img(tmp_path, target_size=target_size)\n",
    "  # Delete temp file\n",
    "  os.remove(tmp_path)\n",
    "  # \n",
    "  img_data = image.img_to_array(img)\n",
    "\n",
    "  # for testing\n",
    "  #plt.imshow(img_data/255.) #Remove or comment\n",
    "  \n",
    "  img_data = np.expand_dims(img_data, axis=0)\n",
    "  img_data = preprocessor(img_data)\n",
    "\n",
    "  \n",
    "\n",
    "  features = model.predict(img_data)\n",
    "  return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocLixz1b8U2t"
   },
   "outputs": [],
   "source": [
    "def register_to_deepfeatures(register,model_name):\n",
    "    \"\"\"\n",
    "    Compute all CNN features    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    register : Series\n",
    "        Serie containing metadata of the image\n",
    "    model_name: str\n",
    "        Name of the pretrained model\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    results : Series \n",
    "        A Pandas Serie contanining all of the features\n",
    "\n",
    "    \"\"\"\n",
    "    img_name = register.Image\n",
    "    Class = register.Class \n",
    "    test_folds = register.filter(regex='Test_Fold')   \n",
    "    \n",
    "    img_dir = imgs_path+os.sep+Class+os.sep    \n",
    "    Name_str = Class+\"_\"+img_name    \n",
    "    \n",
    "    img_path = img_dir+img_name\n",
    "\n",
    "        \n",
    "    img = imread(img_path)\n",
    "\n",
    "    crop = crop_image(img,register.Rectangle)\n",
    "\n",
    "    \n",
    "    basic_values = pd.Series([Name_str,Class],[\"Name\",\"Class\"])\n",
    "    \n",
    "    features = extract_features(crop,model_name)\n",
    "    names = [f\"{model_name}_{i}\" for i in range(len(features))]\n",
    "\n",
    "    \n",
    "\n",
    "    results = pd.Series(features,names)\n",
    "\n",
    "    return pd.concat((basic_values,test_folds,results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained models (VT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing PyTorch Image Models\n",
    "!pip install timm\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# vit_large_patch16_224, image_size 224, features 1024\n",
    "model_vt_large = timm.create_model('vit_large_patch16_224', pretrained=True)\n",
    "model_vt_base = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def register_image_resized(register):\n",
    "    \"\"\"\n",
    "    Resizes and reformats data to be compatible with TIMM\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    img_name = register.Image\n",
    "    Class = register.Class \n",
    "    test_folds = register.filter(regex='Test_Fold')   \n",
    "    \n",
    "    img_dir = imgs_path+os.sep+Class+os.sep    \n",
    "    Name_str = Class+\"_\"+img_name    \n",
    "    \n",
    "    img_path = img_dir+img_name\n",
    "\n",
    "        \n",
    "    img = imread(img_path)\n",
    "\n",
    "    img = crop_image(img,register.Rectangle)\n",
    "    img = resize(img,(224,224,3))\n",
    "\n",
    "    ## formatear de (224,224,3) a (3,224,224) \n",
    "    img = np.concatenate((img[:,:,0],img[:,:,1],img[:,:,2])).reshape((3,224,224))\n",
    "    #Tensor images with a float dtype are expected to have values in [0, 1)\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghkau5BjFs1n"
   },
   "source": [
    "# Check and generate new datasets\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2b4o_OoA2vL",
    "outputId": "1bd239fd-38d4-4964-e7a8-72b3267cbb33"
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def check_files(datasets_file,results_file):\n",
    "  \"\"\"\n",
    "    Check if the datasets and the results files have been generated  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_folder : str\n",
    "        Path of the folder where datasets and results are saved\n",
    "    datasets_file: str\n",
    "        Name of dataset file\n",
    "    results_file: str\n",
    "        Name results file\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "  \"\"\"\n",
    "  datasets_file_path = complete_output_folder_path+datasets_file\n",
    "  results_file_path = complete_output_folder_path+results_file\n",
    "\n",
    "  # if output_folder dont exits, it is created\n",
    "  # empty datasets and results files are also created\n",
    "  if not os.path.isdir(complete_output_folder_path):\n",
    "    os.makedirs(complete_output_folder_path)\n",
    "    print(f\"Created {complete_output_folder_path}\")\n",
    "    datasets_dict = dict()\n",
    "    results_dict = dict()\n",
    "    with open(datasets_file_path, 'wb') as handle:\n",
    "      pickle.dump(datasets_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(results_file_path, 'wb') as handle:\n",
    "      pickle.dump(results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  # if the output_folder exists, the datasets and results files are checked\n",
    "  else:\n",
    "    check_status(datasets_file_path, results_file_path)\n",
    "\n",
    "  \n",
    "def check_status(datasets_file_path, results_file_path):\n",
    "  \"\"\"\n",
    "    Check the status of datasets and the results files  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets_file_path: str\n",
    "        full path of dataset file\n",
    "    datasets_file_path: str\n",
    "        full path results file\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Checkeando\")\n",
    "  with open(datasets_file_path, 'rb') as handle:\n",
    "      datasets_dict = pickle.load(handle)\n",
    "\n",
    "      for key in datasets_dict:\n",
    "        print(key,datasets_dict[key].shape)\n",
    "\n",
    "  with open(results_file_path, 'rb') as handle:\n",
    "      results_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "check_files(datasets_file,results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbxTY_WzfFTg"
   },
   "outputs": [],
   "source": [
    "def generate_datasets(type_dataset,df_info):\n",
    "  \"\"\"\n",
    "    Generate datasets  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    type_dataset: str\n",
    "        Type of features to be computed\n",
    "    df_info: DataFrame\n",
    "        Dataframe with image metadata\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Dataset : DataFrame\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  deep_features = [\"InceptionV3\", \"Xception\", \"VGG16\",\"VGG19\",\"ResNet50\",\n",
    "               \"ResNet101\",\"ResNet152\",\"InceptionResNet\",\"MobileNet\",\n",
    "               \"NASMobile\",\"NASLarge\",\"Dense121\",\"Dense169\",\"Dense201\"]\n",
    "\n",
    "  if type_dataset == \"LBP\":\n",
    "    return df_info.apply(register_to_lbp_features,axis=1)\n",
    "  elif type_dataset == \"Morpho\":\n",
    "    return df_info.apply(register_to_morpho_features,axis=1)\n",
    "  elif type_dataset == \"EFD\":\n",
    "    return df_info.apply(register_to_efd_features,axis=1)\n",
    "  elif type_dataset == \"Hu\":\n",
    "    return df_info.apply(register_to_hu_features,axis=1)\n",
    "  elif type_dataset == \"Zernike5\":\n",
    "    return df_info.apply(lambda x: register_to_zernike_features(x, radious=5),axis=1)\n",
    "  elif type_dataset == \"Zernike10\":\n",
    "    return df_info.apply(lambda x: register_to_zernike_features(x, radious=10),axis=1)\n",
    "  elif type_dataset == \"pftas\":\n",
    "    return df_info.apply(register_to_pftas_features,axis=1)\n",
    "  elif type_dataset == \"Haralick\":\n",
    "    return df_info.apply(register_to_haralick_features,axis=1)\n",
    "\n",
    "  elif type_dataset in deep_features:\n",
    "      return df_info.apply(lambda x: register_to_deepfeatures(x, type_dataset),\n",
    "                                        axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXFojNnNnTL1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Comenta mejor\n",
    "def generate_save_datasets(datasets_file,repeat_dict,test=False):\n",
    "  \"\"\"\n",
    "    Generate datasets and serialize them in disks \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_folder: str\n",
    "        Path of the output folder\n",
    "    datasets_file: str\n",
    "        Name of the datasets file\n",
    "    repeat_dict: Dict\n",
    "        Dictionary that defines which features are to be recalculated\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  dataset_types = list(repeat_dict.keys())\n",
    "\n",
    "  datasets_file_path = complete_output_folder_path+datasets_file\n",
    "  \n",
    "  datasets_dict = None\n",
    "  \n",
    "  with open(datasets_file_path, 'rb') as handle:\n",
    "      datasets_dict = pickle.load(handle)\n",
    "    \n",
    "  # Removes classes with to few examples\n",
    "  classes_with_few_samples = [\"Trilobate\"]\n",
    "  df_info = datasets_dict.get(\"info\") \n",
    "  #df_info = None\n",
    "  if df_info is None:\n",
    "      df_info = create_image_info_df(csvs_path,\n",
    "                                     ignore_classes=classes_with_few_samples)\n",
    "      datasets_dict[\"info\"] = df_info\n",
    "\n",
    "      if not test:\n",
    "        with open(datasets_file_path, 'wb') as handle:\n",
    "            pickle.dump(datasets_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Saving info\")\n",
    "  \n",
    "  if test:\n",
    "    df_info = df_info.head(10)\n",
    "\n",
    "  \n",
    "  # checks if a feature type is previously computed or is needed to computed again\n",
    "  for data_type in dataset_types:\n",
    "    if not data_type in datasets_dict or repeat_dict[data_type]:\n",
    "      print(f\"Computing {data_type}\")\n",
    "      datasets_dict[data_type] = generate_datasets(data_type,df_info)\n",
    "      if not test:\n",
    "        with open(datasets_file_path, 'wb') as handle:\n",
    "          pickle.dump(datasets_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "          print(f\"Saving {data_type}\")\n",
    "    elif data_type in datasets_dict and not repeat_dict[data_type]:\n",
    "      print(f\"{data_type} already computed\")\n",
    "     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3fmJPRIZGeF",
    "outputId": "da6ff2a2-2131-40ef-fe1f-7e008ea5eb60"
   },
   "outputs": [],
   "source": [
    "generate_save_datasets(datasets_file,repeat_dict)\n",
    "clear_output()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Vision Transformer Features\n",
    "\n",
    "in July 2022 this modification allows to obtain features calculated by the last layer of a pre-trained Vision Transformer.\n",
    "\n",
    "Two new datasets (df_large, df_base) are generated, which can be added to the dataset dictionary and used in the rest of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = None\n",
    "with open(complete_output_folder_path+datasets_file, 'rb') as handle:\n",
    "    datasets_dict = pickle.load(handle)\n",
    "    df_info = datasets_dict.get(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract Features using pretrained vision transformers\n",
    "'''\n",
    "larges = []\n",
    "bases = []\n",
    "\n",
    "for i in range(df_info.shape[0]):\n",
    "  if i % 50==0 and not i ==0:\n",
    "    print(i)\n",
    "  imgs = []\n",
    "  imgs.append(register_image_resized(df_info.iloc[i]))\n",
    "  imgs = np.array(imgs)\n",
    "\n",
    "  print(\"(L)\",end=\"\")\n",
    "  large_feats = model_vt_large.forward_features(torch.from_numpy(imgs))\n",
    "  print(\"(B)\",end=\" \")\n",
    "  base_feats = model_vt_base.forward_features(torch.from_numpy(imgs))\n",
    "  \n",
    "  large_feats = large_feats.cpu().detach().numpy()\n",
    "  base_feats = base_feats.cpu().detach().numpy()\n",
    "\n",
    "  larges.append(large_feats)\n",
    "  bases.append(base_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into dataframe\n",
    "df_base = pd.DataFrame(np.array(list(map(lambda x: x[0], bases))))\n",
    "df_large = pd.DataFrame(np.array(list(map(lambda x: x[0], larges))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metadata info\n",
    "print(\"Creating dataframes (base)\")\n",
    "df_base.columns = [f\"VT_base{i}\" for i in range(df_base.shape[1])]\n",
    "\n",
    "df_info0 = df_info.copy()\n",
    "df_info0[\"Name\"] = df_info0[\"Class\"]+\"_\"+df_info0[\"Image\"]\n",
    "df_info1 = df_info0[[\"Name\",\"Class\"]]\n",
    "df_info2 = df_info0.filter(regex='Test_Fold')\n",
    "\n",
    "df_base = pd.concat((df_info1,df_info2,df_base),axis=1)\n",
    "\n",
    "print(\"Creating dataframes (large)\")\n",
    "df_large.columns = [f\"VT_large{i}\" for i in range(df_large.shape[1])]\n",
    "df_large = pd.concat((df_info1,df_info2,df_large),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in drive. \n",
    "# If the dataset is saved it is not necessary to execute the previous cells\n",
    "handle = open(\"df_base.obj\", 'wb')\n",
    "pickle.dump(df_base, handle)\n",
    "handle = open(\"df_large.obj\", 'wb')\n",
    "pickle.dump(df_large, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAQrZfO42zS7"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Q-ppdlm9RiM",
    "outputId": "9790bbd4-0406-403b-8b19-61ee15bc8616"
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "\n",
    "# Load computed datasets\n",
    "def load_datasets():\n",
    "  datasets_file_path = complete_output_folder_path+datasets_file\n",
    "  datasets_dict = None\n",
    "  with open(datasets_file_path, 'rb') as handle:\n",
    "      datasets_dict = pickle.load(handle)\n",
    "\n",
    "  features = list(datasets_dict.keys())\n",
    "  if \"info\" not in features:\n",
    "    print(\"Metadata not calculated\")\n",
    "  else:\n",
    "    features.remove(\"info\")\n",
    "    if len(features) == 0:\n",
    "      print(\"There is not features computed\")\n",
    "    else:\n",
    "      for clave in datasets_dict:\n",
    "        print(clave,datasets_dict[clave].shape)\n",
    "  return datasets_dict\n",
    "    \n",
    "  \n",
    "\n",
    "data_dict = load_datasets()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding ViT features to the data_dict\n",
    "\n",
    "The rest of the code works with datasets stored in the dataset dictionary.\n",
    "\n",
    "If we want to work with the attributes extracted by Vision Transformers we must add these datasets to the datasets dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = open(\"df_base.obj\", 'rb')\n",
    "df_base = pickle.load(handle)\n",
    "handle = open(\"df_large.obj\", 'rb')\n",
    "df_large = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['vt_large']=df_large \n",
    "data_dict['vt_base']=df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlohsF8gAidg"
   },
   "source": [
    "# Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdGTDJyjAlEg",
    "outputId": "4b2aa6b8-d88d-4dae-fee6-3b614f4015bd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset concatenations\n",
    "'''\n",
    "\n",
    "def combine_datasets(df1,df2):\n",
    "  columns1 = df1.columns\n",
    "  columns2 = df2.columns\n",
    "  att_columns1 = [c for c in columns1]\n",
    "  att_columns2 = [c for c in columns2 if not (\"Test_Fold\" in c or c in [\"Name\",\"Class\"])]\n",
    "\n",
    "  df_comb = pd.concat((df1[att_columns1],df2[att_columns2]),axis=1)\n",
    "\n",
    "  return df_comb\n",
    "\n",
    "\n",
    "df_morpho_hu = combine_datasets(data_dict[\"Morpho\"],data_dict[\"Hu\"])\n",
    "df_morpho_hu_EFD = combine_datasets(df_morpho_hu,data_dict[\"EFD\"])\n",
    "df_morpho_EFD = combine_datasets(data_dict[\"Morpho\"],data_dict[\"EFD\"])\n",
    "\n",
    "df_nas_inc = combine_datasets(data_dict[\"NASLarge\"],data_dict[\"InceptionResNet\"])\n",
    "df_nas_inc_dense = combine_datasets(df_nas_inc,data_dict[\"Dense169\"])\n",
    "\n",
    "df_morpho_nas = combine_datasets(data_dict[\"Morpho\"],data_dict[\"NASLarge\"])\n",
    "df_morpho_pftas = combine_datasets(data_dict[\"Morpho\"],data_dict[\"pftas\"])\n",
    "\n",
    "data_dict[\"Morpho/Hu\"] = df_morpho_hu\n",
    "data_dict[\"Morpho/EFD\"] = df_morpho_EFD\n",
    "data_dict[\"Morpho/Hu/EFD\"] = df_morpho_hu_EFD\n",
    "\n",
    "data_dict[\"NASLarge/InceptionResNet\"] = df_nas_inc\n",
    "data_dict[\"NASLarge/InceptionResNet/Dense169\"] = df_nas_inc_dense\n",
    "\n",
    "data_dict[\"Morpho/NASLarge\"] = df_morpho_nas\n",
    "data_dict[\"Morpho/pftas\"] = df_morpho_pftas\n",
    "\n",
    "\n",
    "df_morpho_hu_EFD_NAS = combine_datasets(data_dict[\"Morpho/Hu/EFD\"],\n",
    "                                        data_dict[\"NASLarge\"])\n",
    "df_morpho_hu_EFD_pftas = combine_datasets(data_dict[\"Morpho/Hu/EFD\"],\n",
    "                                        data_dict[\"pftas\"])\n",
    "data_dict[\"Morpho/Hu/EFD/NASLarge\"] = df_morpho_hu_EFD_NAS\n",
    "data_dict[\"Morpho/Hu/EFD/pftas\"] = df_morpho_hu_EFD_pftas\n",
    "\n",
    "# vtb/nas \n",
    "df_vtb_nas = combine_datasets(data_dict[\"vt_base\"],data_dict[\"NASLarge\"])\n",
    "data_dict[\"vt_base/NASLarge\"] = df_vtb_nas\n",
    "\n",
    "# vtb/nas/morpho \n",
    "df_vtb_nas_morpho = combine_datasets(data_dict[\"vt_base/NASLarge\"],data_dict[\"Morpho\"])\n",
    "data_dict[\"vt_base/NASLarge/Morpho\"] = df_vtb_nas_morpho\n",
    "\n",
    "\n",
    "# vtb/nas/inc\n",
    "df_vtb_nas_inc = combine_datasets(data_dict[\"vt_base/NASLarge\"],data_dict[\"InceptionResNet\"])\n",
    "data_dict[\"vt_base/NASLarge/InceptionResNet\"] = df_vtb_nas_inc\n",
    "# vtb/nas/inc/morpho/hu\n",
    "df_vtb_nas_inc_morpho_hu = combine_datasets(data_dict[\"vt_base/NASLarge/InceptionResNet\"],data_dict[\"Morpho/Hu\"])\n",
    "data_dict[\"vt_base/NASLarge/InceptionResNet/Morpho/Hu\"] = df_vtb_nas_inc_morpho_hu\n",
    "\n",
    "\n",
    "df_vtb_morpho = combine_datasets(data_dict[\"vt_base\"],data_dict[\"Morpho\"])\n",
    "data_dict[\"vt_base/Morpho\"] = df_vtb_morpho\n",
    "\n",
    "\n",
    "\n",
    "df_vtb_morpho_hu = combine_datasets(data_dict[\"vt_base/Morpho\"],data_dict[\"Hu\"])\n",
    "data_dict[\"vt_base/Morpho/Hu\"] = df_vtb_morpho_hu\n",
    "df_vtb_morpho_hu_efd = combine_datasets(data_dict[\"vt_base/Morpho/Hu\"],data_dict[\"EFD\"])\n",
    "data_dict[\"vt_base/Morpho/Hu/EFD\"] = df_vtb_morpho_hu_efd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets_file_path = complete_output_folder_path+datasets_file\n",
    "  \n",
    "with open(datasets_file_path, 'wb') as handle:\n",
    "  pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  print(f\"Saving info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More July 2022\n",
    "\n",
    "df_vtbs = combine_datasets(data_dict[\"vt_large\"],data_dict[\"vt_base\"])\n",
    "data_dict[\"vt_large/vt_base\"] = df_vtbs\n",
    "\n",
    "df_vts_morpho = combine_datasets(data_dict[\"vt_large/vt_base\"],data_dict[\"Morpho\"])\n",
    "data_dict[\"vt_large/vt_base/Morpho\"] = df_vts_morpho\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_vts_morpho_hu = combine_datasets(data_dict[\"vt_large/vt_base/Morpho\"],data_dict[\"Hu\"])\n",
    "data_dict[\"vt_large/vt_base/Morpho/Hu\"] = df_vts_morpho_hu\n",
    "df_vts_morpho_hu_efd = combine_datasets(data_dict[\"vt_large/vt_base/Morpho/Hu\"],data_dict[\"EFD\"])\n",
    "data_dict[\"vt_large/vt_base/Morpho/Hu/EFD\"] = df_vts_morpho_hu_efd\n",
    "\n",
    "\n",
    "datasets_file_path = complete_output_folder_path+datasets_file\n",
    "  \n",
    "with open(datasets_file_path, 'wb') as handle:\n",
    "  pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  print(f\"Saving info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqBvs01perJa",
    "outputId": "a014d833-6bac-4fce-a407-ed87379d42ac"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load computed results \n",
    "\n",
    "All the predictions made are stored in a results dictionary.\n",
    "'''\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "def load_results():\n",
    "  results_file_path = complete_output_folder_path+results_file\n",
    "  results_dict = None\n",
    "  if os.path.isfile(results_file_path):\n",
    "    with open(results_file_path, 'rb') as handle:\n",
    "        results_dict = pickle.load(handle)\n",
    "\n",
    "    results = list(results_dict.keys())\n",
    "    for clave in results_dict:\n",
    "        print(clave)\n",
    "    return results_dict\n",
    "  else:\n",
    "    return {} \n",
    "\n",
    "res_dict = load_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers: creation and parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ki-B6b-3Tx8"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-532uyfEUsCf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Definition of the SVM parameter search\n",
    "'''\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid_svm = dict(gamma=gamma_range, C=C_range)\n",
    "nested_cv = 5\n",
    "\n",
    "grid_svm = GridSearchCV(SVC(), param_grid=param_grid_svm, cv=nested_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwM4CFYdUs9Q"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Definition of the MLP parameter search\n",
    "'''\n",
    "alpha_range = np.logspace(-5, -1, 5)\n",
    "hidden_layer_sizes_range=[(50,),(100,),(200,),(500,),(1000,)]\n",
    "\n",
    "param_grid_mlp = dict(alpha=alpha_range, hidden_layer_sizes=hidden_layer_sizes_range)\n",
    "\n",
    "\n",
    "grid_mlp = GridSearchCV(MLPClassifier(max_iter=1000,\n",
    "                                      early_stopping=True), param_grid=param_grid_mlp, cv=nested_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7jqdeLrlEqd"
   },
   "outputs": [],
   "source": [
    "def compute_predictions_per_fold_repetition(df, model,\n",
    "                                        dict_all_results,dict_model_results,\n",
    "                                        model_name,data_name):\n",
    "  '''\n",
    "  Uses a dataset previously divided into several train and test partitions\n",
    "  Trains a model with the train_cv part of the dataset \n",
    "  and obtains the predictions with the test_cv part\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  df: DataFrame\n",
    "      DataFrame containing, features, classes and partitions\n",
    "  model: scikit_model\n",
    "      model to be trained\n",
    "  num_folds: int\n",
    "      number of folds in the cross validation\n",
    "  \n",
    "  Return\n",
    "  -------\n",
    "  List \n",
    "      Dict of number_of_fold:(y_true,y_pred).\n",
    "      \n",
    "  '''\n",
    "    \n",
    "  results_file_path = complete_output_folder_path+results_file\n",
    "  \n",
    "  \n",
    "  if dict_model_results is None:\n",
    "    dict_model_results = {}\n",
    "  \n",
    "  columns = df.columns \n",
    "  att_columns = [c for c in columns if not (\"Test_Fold\" in c or c in [\"Name\",\"Class\"])]\n",
    "  repetitions = [int(c.replace('Test_Fold','')) for c in columns if\"Test_Fold\" in c]\n",
    "\n",
    "  # refactoriza lo de abajo\n",
    "  for repetition in repetitions:\n",
    "\n",
    "    partitions = np.sort(df[f\"Test_Fold{repetition}\"].unique())\n",
    "    for partition in partitions:\n",
    "      # ojo checkear\n",
    "\n",
    "      if not repetition in dict_model_results or not partition in dict_model_results[repetition]:\n",
    "        ######################################################\n",
    "        # Data split\n",
    "        df_train = df[df[f\"Test_Fold{repetition}\"]!=partition]\n",
    "        df_test = df[df[f\"Test_Fold{repetition}\"]==partition]\n",
    "\n",
    "        # Remove not relevant attributes\n",
    "        X_train = df_train[att_columns].values\n",
    "        y_train = df_train.Class.values\n",
    "\n",
    "        X_test = df_test[att_columns].values\n",
    "        y_test = df_test.Class.values\n",
    "\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_preds = model.predict(X_test)\n",
    "\n",
    "        if dict_model_results.get(repetition) is None:\n",
    "          dict_model_results[repetition] = {}\n",
    "\n",
    "        dict_model_results[repetition][partition]=(y_test,y_preds)\n",
    "        dict_all_results[model_name,data_name]=dict_model_results\n",
    "        with open(results_file_path, 'wb') as handle:\n",
    "              pickle.dump(dict_all_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "              print(f\"Sav R{repetition}F{partition}\", end=' - ')\n",
    "        ##########################################################\n",
    "      else:\n",
    "        print(f\"Rec R{repetition}F{partition}\", end=' - ')\n",
    "    print(\".\")\n",
    "  \n",
    "  \n",
    "  print('OK')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of classifiers that will be evaluated with each and every one of the previously stored datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K043pPg_WI4D"
   },
   "outputs": [],
   "source": [
    "cls_names = [\n",
    "             \"Nearest Neighbors\",\n",
    "             \"SVM\", \n",
    "             \"MLP\",\n",
    "             \"LogisticRegression\",\n",
    "             \"Decision Tree\", \n",
    "             \"Random Forest\",\n",
    "             \"Gradient Boosting Trees\"             \n",
    "             ]\n",
    "\n",
    "classifiers = [\n",
    "    make_pipeline(StandardScaler(), KNeighborsClassifier(3)),\n",
    "    make_pipeline(StandardScaler(), grid_svm),\n",
    "    make_pipeline(StandardScaler(), grid_mlp),\n",
    "    make_pipeline(StandardScaler(), LogisticRegression(max_iter=5000)),    \n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    RandomForestClassifier(random_state=0, n_estimators=100),\n",
    "    GradientBoostingClassifier(random_state=0, n_estimators=100)    \n",
    "]\n",
    "\n",
    "\n",
    "# res_dict = {} # To force the repetition of the experiments delete the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions by cross validation.\n",
    "\n",
    "execution of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YzsI9YNUWXsf",
    "outputId": "cda2ada7-caf1-4fb3-fe50-651512c33c09",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "features = list(data_dict.keys())\n",
    "features.remove(\"info\")\n",
    "\n",
    "for feature in features:\n",
    "  datasets.append((feature,data_dict[feature]))\n",
    "\n",
    "\n",
    "for dataset_name,dataset in datasets:\n",
    "  print(dataset_name)\n",
    "  for cls_name,cls in zip(cls_names,classifiers):\n",
    "    print(cls_name)\n",
    "    # obtiene resultados parciales si los hubiese\n",
    "    results = res_dict.get((cls_name, dataset_name))\n",
    "    # actualiza los resultados y serializa resultados\n",
    "    compute_predictions_per_fold_repetition(dataset, cls,res_dict,results,cls_name,dataset_name)\n",
    "    \n",
    "res_dict = load_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgymaqQGQw0q",
    "outputId": "3039e023-96c1-49ef-e9c3-f5947ee88e46"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "Helper functions to implement multi-view stacking\n",
    "'''\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, X):\n",
    "        col_list = []\n",
    "        for c in self.cols:\n",
    "            col_list.append(X[:, c:c+1])\n",
    "        return np.concatenate(col_list, axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "df1 = data_dict[\"Morpho/Hu\"]\n",
    "\n",
    "def get_num_atts(df):\n",
    "  columns = df.columns \n",
    "  att_columns = [c for c in columns if not (\"Test_Fold\" in c or c in [\"Name\",\"Class\"])]\n",
    "  return len(att_columns)\n",
    "\n",
    "def get_X(df):\n",
    "  columns = df.columns \n",
    "  att_columns = [c for c in columns if not (\"Test_Fold\" in c or c in [\"Name\",\"Class\"])]\n",
    "  return df[att_columns].values\n",
    "\n",
    "def get_y(df):\n",
    "  return df[\"Class\"].values\n",
    "\n",
    "\n",
    "n_atts_morpho = get_num_atts(data_dict[\"Morpho\"])\n",
    "n_atts_hu = get_num_atts(data_dict[\"Hu\"])\n",
    "n_atts_efd = get_num_atts(data_dict[\"EFD\"])\n",
    "n_atts_nas = get_num_atts(data_dict[\"NASLarge\"])\n",
    "n_atts_inc = get_num_atts(data_dict[\"InceptionResNet\"])\n",
    "n_atts_dense = get_num_atts(data_dict[\"Dense169\"])\n",
    "n_atts_pftas = get_num_atts(data_dict[\"pftas\"])\n",
    "n_atts_vtbase = get_num_atts(data_dict[\"vt_base\"])\n",
    "n_atts_vtlarge = get_num_atts(data_dict[\"vt_large\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lig-vmCpBCeX"
   },
   "outputs": [],
   "source": [
    "def get_selected_cls():\n",
    "  return GridSearchCV(SVC(), param_grid=param_grid_svm, cv=nested_cv)\n",
    "  \n",
    "\n",
    "def get_meta_cls():\n",
    "  return LogisticRegression(max_iter=5000)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSDLN2VP5pkf"
   },
   "outputs": [],
   "source": [
    "######################Morpho/Hu###################\n",
    "estimators_morpho_hu = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_selected_cls())),\n",
    "                         ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_hu)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "###################Morpho/Hu/EFD######################\n",
    "estimators_morpho_hu_efd = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_selected_cls())),\n",
    "                         ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_hu)),get_selected_cls())),\n",
    "                        ('solo_efd', make_pipeline(ColumnExtractor(range(n_atts_morpho+n_atts_hu,\n",
    "                                                                         n_atts_morpho+n_atts_hu+n_atts_efd)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "######################Morpho/pftas###################\n",
    "estimators_morpho_pftas = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_selected_cls())),\n",
    "                         ('solo_pftas', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_pftas)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "######################Morpho/NasLarge###################\n",
    "estimators_morpho_nas = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_selected_cls())),\n",
    "                         ('solo_nas', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_nas)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "######################NasLarge/InceptionResNet###################\n",
    "estimators_nas_inc = [('solo_nas', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_nas)), get_selected_cls())),\n",
    "                         ('solo_inc', make_pipeline(ColumnExtractor(range(n_atts_nas,\n",
    "                                                                         n_atts_nas+n_atts_inc)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "######################NasLarge/InceptionResNet/Dense###################\n",
    "estimators_nas_inc_dense = [('solo_nas', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_nas)), get_selected_cls())),\n",
    "                         ('solo_inc', make_pipeline(ColumnExtractor(range(n_atts_nas,\n",
    "                                                                         n_atts_nas+n_atts_inc)),get_selected_cls())),\n",
    "                      ('solo_dense', make_pipeline(ColumnExtractor(range(n_atts_nas+n_atts_inc,\n",
    "                                                                         n_atts_nas+n_atts_inc+n_atts_dense)),get_selected_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "\n",
    "######################Morpho/Hu###################\n",
    "clf_morpho_hu_stack = StackingClassifier(estimators=estimators_morpho_hu, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "#####################Morpho/Hu/EFD####################\n",
    "clf_morpho_hu_efd_stack = StackingClassifier(estimators=estimators_morpho_hu_efd, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "######################Morpho/pftas###################\n",
    "clf_morpho_pftas_stack = StackingClassifier(estimators=estimators_morpho_pftas, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "######################Morpho/NasLarge###################\n",
    "clf_morpho_nas_stack = StackingClassifier(estimators=estimators_morpho_nas, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "######################NasLarge/InceptionResNet###################\n",
    "clf_nas_inc_stack = StackingClassifier(estimators=estimators_nas_inc, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "######################NasLarge/InceptionResNet/Dense###################\n",
    "clf_nas_inc_dense_stack = StackingClassifier(estimators=estimators_nas_inc_dense, \n",
    "                                         final_estimator=get_meta_cls())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0hZAs-W2yXx",
    "outputId": "2466825f-61d4-4543-ed97-6897b7cd983e"
   },
   "outputs": [],
   "source": [
    "res_dict = load_results()\n",
    "\n",
    "######################Morpho/Hu###################\n",
    "# gets partial results from previous runs\n",
    "results = res_dict.get((\"(Morpho/Hu)Stack\", \"Morpho/Hu\"))\n",
    "# update and save results\n",
    "print(\"(Morpho/Hu)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/Hu\"], \n",
    "                                        clf_morpho_hu_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/Hu)Stack\",\"Morpho/Hu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkrLU1uRR5fB",
    "outputId": "7fceccad-9605-4235-b5af-d243280a403e"
   },
   "outputs": [],
   "source": [
    "######################Morpho/Hu/EFD###################\n",
    "# gets partial results from previous runs\n",
    "results = res_dict_stacking.get((\"(Morpho/Hu/EFD)Stack\", \"Morpho/Hu/EFD\"))\n",
    "# update and save results\n",
    "print(\"(Morpho/Hu/EFD)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/Hu/EFD\"], \n",
    "                                        clf_morpho_hu_efd_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/Hu/EFD)Stack\",\"Morpho/Hu/EFD\")\n",
    "\n",
    "######################Morpho/pftas###################\n",
    "# gets partial results from previous runs\n",
    "results = res_dict_stacking.get((\"(Morpho/pftas)Stack\", \"Morpho/pftas\"))\n",
    "# update and save results\n",
    "print(\"(Morpho/pftas)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/pftas\"], \n",
    "                                        clf_morpho_pftas_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/pftas)Stack\",\"Morpho/pftas\")\n",
    "\n",
    "######################Morpho/NasLarge###################\n",
    "results = res_dict_stacking.get((\"(Morpho/NASLarge)Stack\", \"Morpho/NASLarge\"))\n",
    "print(\"(Morpho/NASLarge)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/NASLarge\"], \n",
    "                                        clf_morpho_nas_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/NASLarge)Stack\",\"Morpho/NASLarge\")\n",
    "\n",
    "######################NasLarge/InceptionResNet###################\n",
    "results = res_dict_stacking.get((\"(NASLarge/InceptionResNet)Stack\", \"NASLarge/InceptionResNet\"))\n",
    "print(\"(NASLarge/InceptionResNet)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"NASLarge/InceptionResNet\"], \n",
    "                                        clf_nas_inc_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(NASLarge/InceptionResNet)Stack\",\"NASLarge/InceptionResNet\")\n",
    "\n",
    "######################NasLarge/InceptionResNet/Dense###################\n",
    "results = res_dict_stacking.get((\"(NASLarge/InceptionResNet/Dense169)Stack\", \"NASLarge/InceptionResNet/Dense169\"))\n",
    "print(\"(NASLarge/InceptionResNet/Dense169)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"NASLarge/InceptionResNet/Dense169\"], \n",
    "                                        clf_nas_inc_dense_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(NASLarge/InceptionResNet/Dense169)Stack\",\"NASLarge/InceptionResNet/Dense169\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Morpho/Hu/EFD/NAS######################\n",
    "estimators_morpho_hu_efd_nas = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_SVM_cls())),\n",
    "                         ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_hu)),get_SVM_cls())),\n",
    "                        ('solo_efd', make_pipeline(ColumnExtractor(range(n_atts_morpho+n_atts_hu,\n",
    "                                                                         n_atts_morpho+n_atts_hu+n_atts_efd)),get_RF_cls())),\n",
    "                        ('solo_nas', make_pipeline(ColumnExtractor(range(n_atts_morpho+n_atts_hu+n_atts_efd,\n",
    "                                                                         n_atts_morpho+n_atts_hu+n_atts_efd+n_atts_nas)),get_Log_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################Morpho/Hu/EFD/pftas ######################\n",
    "estimators_morpho_hu_efd_pftas = [('solo_morpho', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                            n_atts_morpho)), get_SVM_cls())),\n",
    "                         ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_morpho,\n",
    "                                                                         n_atts_morpho+n_atts_hu)),get_SVM_cls())),\n",
    "                        ('solo_efd', make_pipeline(ColumnExtractor(range(n_atts_morpho+n_atts_hu,\n",
    "                                                                         n_atts_morpho+n_atts_hu+n_atts_efd)),get_RF_cls())),\n",
    "                        ('solo_pftas', make_pipeline(ColumnExtractor(range(n_atts_morpho+n_atts_hu+n_atts_efd,\n",
    "                                                                         n_atts_morpho+n_atts_hu+n_atts_efd+n_atts_pftas)),get_SVM_cls()))\n",
    "                         ]\n",
    "\n",
    "\n",
    "\n",
    "clf_morpho_hu_efd_nas_stack = StackingClassifier(estimators=estimators_morpho_hu_efd_nas, \n",
    "                                         final_estimator=get_Log_cls())\n",
    "\n",
    "clf_morpho_hu_efd_pftas_stack = StackingClassifier(estimators=estimators_morpho_hu_efd_pftas, \n",
    "                                         final_estimator=get_Log_cls())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Morpho/Hu/EFD/NASLarge###################\n",
    "results = res_dict.get((\"(Morpho/Hu/EFD/NASLarge)Stack\", \"Morpho/Hu/EFD/NASLarge\"))\n",
    "print(\"(Morpho/Hu/EFD/NASLarge)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/Hu/EFD/NASLarge\"], \n",
    "                                        clf_morpho_hu_efd_nas_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/Hu/EFD/NASLarge)Stack\",\"Morpho/Hu/EFD/NASLarge\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = res_dict.get((\"(Morpho/Hu/EFD/pftas)Stack\", \"Morpho/Hu/EFD/pftas\"))\n",
    "print(\"(Morpho/Hu/EFD/pftas)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"Morpho/Hu/EFD/pftas\"], \n",
    "                                        clf_morpho_hu_efd_pftas_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(Morpho/Hu/EFD/pftas)Stack\",\"Morpho/Hu/EFD/pftas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisión Julio 22 #######################################################\n",
    "###########################################################################\n",
    "#1) vt_base/Morpho           Log/SVM                     \n",
    "#2) vt_base/Morpho/Hu        Log/SVM/SVM                     \n",
    "#3) vt_base/Morpho/Hu/EFD    Log/SVM/SVM/RF                     \n",
    "#4) vt_base/NASLarge         Log/Log\n",
    "#5) vt_base/NASLarge/Morpho  Log/Log/SVM                     \n",
    "estimators_vt_morpho = [('solo_vt', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                        n_atts_vtbase)), \n",
    "                                                  get_Log_cls())),\n",
    "                        ('solo_morpho', make_pipeline(ColumnExtractor(range(n_atts_vtbase,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho)), \n",
    "                                                      get_SVM_cls()))]\n",
    "\n",
    "estimators_vt_morpho_hu = [('solo_vt', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                        n_atts_vtbase)), \n",
    "                                                     get_Log_cls())),\n",
    "                           ('solo_morpho', make_pipeline(ColumnExtractor(range(n_atts_vtbase,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho)), \n",
    "                                                      get_SVM_cls())),\n",
    "                           ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_vtbase+n_atts_morpho,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho+n_atts_hu)), \n",
    "                                                      get_SVM_cls()))]\n",
    "estimators_vt_morpho_hu_efd = [('solo_vt', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                        n_atts_vtbase)), \n",
    "                                                         get_Log_cls())),\n",
    "                               ('solo_morpho', make_pipeline(ColumnExtractor(range(n_atts_vtbase,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho)), \n",
    "                                                      get_SVM_cls())),\n",
    "                               ('solo_hu', make_pipeline(ColumnExtractor(range(n_atts_vtbase+n_atts_morpho,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho+n_atts_hu)), \n",
    "                                                      get_SVM_cls())),\n",
    "                               ('solo_efd', make_pipeline(ColumnExtractor(range(n_atts_vtbase+n_atts_morpho+n_atts_hu,\n",
    "                                                                            n_atts_vtbase+n_atts_morpho+n_atts_hu+n_atts_efd)), \n",
    "                                                      get_RF_cls()))\n",
    "                              ]\n",
    "\n",
    "estimators_vt_nas = [('solo_vt', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                        n_atts_vtbase)), \n",
    "                                                  get_Log_cls())),\n",
    "                    ('solo_nas', make_pipeline(ColumnExtractor(range(n_atts_vtbase,\n",
    "                                                                            n_atts_vtbase+n_atts_nas)), \n",
    "                                                      get_Log_cls()))]\n",
    "estimators_vt_nas_morpho = [('solo_vt', make_pipeline(ColumnExtractor(range(0,\n",
    "                                                                        n_atts_vtbase)), \n",
    "                                                  get_Log_cls())),\n",
    "                            ('solo_nas', make_pipeline(ColumnExtractor(range(n_atts_vtbase,\n",
    "                                                                            n_atts_vtbase+n_atts_nas)), \n",
    "                                                      get_Log_cls())),\n",
    "                            ('solo_morpho', make_pipeline(ColumnExtractor(range(n_atts_vtbase+n_atts_nas,\n",
    "                                                                            n_atts_vtbase+n_atts_nas+n_atts_morpho)), \n",
    "                                                      get_SVM_cls()))\n",
    "                           ]\n",
    "\n",
    "clf_vt_morpho_stack = StackingClassifier(estimators=estimators_vt_morpho,final_estimator=get_Log_cls())\n",
    "clf_vt_morpho_hu_stack = StackingClassifier(estimators=estimators_vt_morpho_hu,final_estimator=get_Log_cls())\n",
    "clf_vt_morpho_hu_efd_stack = StackingClassifier(estimators=estimators_vt_morpho_hu_efd,final_estimator=get_Log_cls())\n",
    "clf_vt_nas_stack = StackingClassifier(estimators=estimators_vt_nas,final_estimator=get_Log_cls())\n",
    "clf_vt_nas_morpho_stack = StackingClassifier(estimators=estimators_vt_nas_morpho,final_estimator=get_Log_cls())\n",
    "\n",
    "res_dict = load_results()\n",
    "\n",
    "\n",
    "######################vt_base/Morpho###################\n",
    "results = res_dict.get((\"(vt_base/Morpho)Stack\", \"vt_base/Morpho\"))\n",
    "print(\"(vt_base/Morpho)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"vt_base/Morpho\"], \n",
    "                                        clf_vt_morpho_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(vt_base/Morpho)Stack\",\"vt_base/Morpho\")\n",
    "\n",
    "######################vt_base/Morpho/hu###################\n",
    "results = res_dict.get((\"(vt_base/Morpho/Hu)Stack\", \"vt_base/Morpho/Hu\"))\n",
    "print(\"(vt_base/Morpho/Hu)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"vt_base/Morpho/Hu\"], \n",
    "                                        clf_vt_morpho_hu_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(vt_base/Morpho/Hu)Stack\",\"vt_base/Morpho/Hu\")\n",
    "\n",
    "######################vt_base/Morpho/Hu/EFD###################\n",
    "results = res_dict.get((\"(vt_base/Morpho/Hu/EFD)Stack\", \"vt_base/Morpho/Hu/EFD\"))\n",
    "print(\"(vt_base/Morpho/Hu/EFD)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"vt_base/Morpho/Hu/EFD\"], \n",
    "                                        clf_vt_morpho_hu_efd_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(vt_base/Morpho/Hu/EFD)Stack\",\"vt_base/Morpho/Hu/EFD\")\n",
    "\n",
    "######################vt_base/NAS###################\n",
    "results = res_dict.get((\"(vt_base/NASLarge)Stack\", \"vt_base/NASLarge\"))\n",
    "print(\"(vt_base/NASLarge)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"vt_base/NASLarge\"], \n",
    "                                        clf_vt_nas_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(vt_base/NASLarge)Stack\",\"vt_base/NASLarge\")\n",
    "\n",
    "######################vt_base/NASLargeMorpho###################\n",
    "results = res_dict.get((\"(vt_base/NASLarge/Morpho)Stack\", \"vt_base/NASLarge/Morpho\"))\n",
    "print(\"(vt_base/NASLarge/Morpho)Stack\")\n",
    "compute_predictions_per_fold_repetition(data_dict[\"vt_base/NASLarge/Morpho\"], \n",
    "                                        clf_vt_nas_morpho_stack,\n",
    "                                        res_dict,\n",
    "                                        results,\n",
    "                                        \"(vt_base/NASLarge/Morpho)Stack\",\"vt_base/NASLarge/Morpho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wop-LZVg7n9b"
   ],
   "name": "CICT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
